<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录，学习，生活。"><title>Spark | Jinzhao Tian</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><div class="darkmode-toggle">🌓</div><script>var prefersDarkMode = window.matchMedia('(prefers-color-scheme: dark)');
var toggle = document.querySelector('.darkmode-toggle');
var html = document.querySelector('html');

html.dataset.dark = localStorage.dark || prefersDarkMode.matches;

toggle.addEventListener('click', () => {
localStorage.dark = !(html.dataset.dark == 'true');
html.dataset.dark = localStorage.dark;
});</script><meta name="generator" content="Hexo 6.1.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark</h1><a id="logo" href="/.">Jinzhao Tian</a><p class="description">As I was said, I was young and stupid.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tags"> 标签</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark</h1><div class="post-meta">2022-04-29<span> | </span><span class="category"><a href="/categories/Big-Data/">Big Data</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-%E6%A8%A1%E5%9D%97"><span class="toc-number">1.</span> <span class="toc-text">Spark 模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-Core"><span class="toc-number">1.1.</span> <span class="toc-text">Spark Core</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL"><span class="toc-number">1.2.</span> <span class="toc-text">Spark SQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-Streaming"><span class="toc-number">1.3.</span> <span class="toc-text">Spark Streaming</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GraphX"><span class="toc-number">1.4.</span> <span class="toc-text">GraphX</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MLlib"><span class="toc-number">1.5.</span> <span class="toc-text">MLlib</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">2.</span> <span class="toc-text">基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD"><span class="toc-number">2.1.</span> <span class="toc-text">RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkContext"><span class="toc-number">2.2.</span> <span class="toc-text">SparkContext</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkSession"><span class="toc-number">2.3.</span> <span class="toc-text">SparkSession</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E6%A8%A1%E5%BC%8F"><span class="toc-number">3.</span> <span class="toc-text">操作模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4"><span class="toc-number">4.</span> <span class="toc-text">作业提交</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">作业流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">提交方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PySpark"><span class="toc-number">5.</span> <span class="toc-text">PySpark</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E6%A8%A1%E5%BC%8F-1"><span class="toc-number">5.1.</span> <span class="toc-text">操作模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9D%97"><span class="toc-number">5.2.</span> <span class="toc-text">基本模块</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-Core-1"><span class="toc-number">5.2.1.</span> <span class="toc-text">Spark Core</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#RDD-%E5%88%9B%E5%BB%BA"><span class="toc-number">5.2.1.1.</span> <span class="toc-text">RDD 创建</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RDD-%E5%B8%B8%E7%94%A8%E5%86%85%E7%BD%AE%E6%96%B9%E6%B3%95"><span class="toc-number">5.2.1.2.</span> <span class="toc-text">RDD 常用内置方法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-SQL-1"><span class="toc-number">5.2.2.</span> <span class="toc-text">Spark SQL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-number">5.2.3.</span> <span class="toc-text">相关聚合函数</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><p>MapReduce 编程模型已经成为主流的分布式编程模型，它极大地方便了编程人员在<strong>不会分布式并行编程</strong>的情况下，将自己的程序运行在分布式系统上。但是 MapReduce 也存在一些缺陷，如高延迟、不支持 DAG 模型、Map 与 Reduce 的中间数据落地等。因此在近两年，社区出现了优化改进 MapReduce 的项目，如交互查询引擎 Impala、支持 DAG 的 TEZ、支持内存计算 Spark 等。Spark 是 UC Berkeley AMP lab 开源的通用并行计算框架，以其先进的设计理念，已经成为社区的热门项目。</p>
<p>Spark 特点如下：</p>
<ul>
<li>Spark 基于内存，尽可能的减少了中间结果写入磁盘和不必要的 sort、shuffle</li>
<li>Spark 对于反复用到的数据进行了缓存</li>
<li>Spark 对于 DAG 进行了高度的优化，具体在于 Spark 划分了不同的 stage 和使用了延迟计算技术</li>
</ul>
<blockquote>
<p>参考：<a href="http://spark.apache.org/docs/latest/">官方文档</a></p>
</blockquote>
<h2 id="Spark-模块"><a href="#Spark-模块" class="headerlink" title="Spark 模块"></a>Spark 模块</h2><p>Spark 力图整合机器学习（MLib）、图算法（GraphX）、流式计算（Spark Streaming）和数据仓库（Spark SQL）等领域，通过计算引擎 Spark，弹性分布式数据集（RDD），架构出一个新的大数据应用平台。</p>
<p><img src="/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Spark-img-1.jpg"></p>
<p>Spark 生态圈以 HDFS、S3、Techyon 为底层存储引擎，以 Yarn、Mesos 和 Standlone 作为资源调度引擎；使用 Spark，可以实现 MapReduce 应用；基于 Spark，Spark SQL 可以实现即席查询，Spark Streaming 可以处理实时应用，MLib 可以实现机器学习算法，GraphX 可以实现图计算，Spark R 可以实现复杂数学计算。</p>
<p>基本模块如下：</p>
<h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h3><p>Spark 的核心功能实现，包含 RDD、任务调度、内存管理、错误恢复、与存储系统交互等模块。</p>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><blockquote>
<p>参考：<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL Guide</a></p>
</blockquote>
<p>提供 SQL 处理能力，便于熟悉关系型数据库操作的工程师进行交互查询。此外，还为熟悉 Hive 开发的用户提供了对 Hive SQL 的支持。</p>
<p>Spark SQL 为了简化 RDD 的开发，提高开发效率，提供了 2 个编程抽象，类似 Spark Core 中的 RDD:</p>
<ol>
<li>DataFrame：DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待。</li>
<li>DataSet：DataSet 是分布式数据集合。DataSet 是 DataFrame 的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter 等等）。</li>
</ol>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p>提供流式计算处理能力，目前支持 Apache Kafka、Apache Flume、Amazon Kinesis 和简单的 TCP 套接字等多种数据源。此外，Spark Streaming 还提供窗口操作用于对一定周期内的流数据进行处理。</p>
<h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><p>提供图计算处理能力，支持分布式，Pregel 提供的 API 可以解决图计算中的常见问题。</p>
<h3 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h3><p>Spark 提供的机器学习库。MLlib 提供了机器学习相关的统计、分类、回归等领域的多种算法实现。其一致的 API 接口大大降低了用户的学习成本。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>Spark 封装一些基本抽象，其中最重要的就是弹性分布式数据集（Resiliennt Distributed Datasets，RDD）。这是 Spark 唯一的数据结构，Spark 的核心是建立在这个统一的抽象上的。</p>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD，Resilient Distributed Datasets（弹性分布式数据集），是 spark 提供的最重要的抽象概念。可以将 RDD 理解为一个<strong>分布式对象集合</strong>，本质上是一个只读的分区记录集合。每个 RDD 可以分成多个分区，每个分区就是一个数据集片段。一个 RDD 的不同分区可以保存到集群中的不同结点上，从而可以在集群中的不同结点上进行并行计算。</p>
<p>RDD 具有容错机制，并且只读不能修改，可以执行确定的转换操作创建新的 RDD。具体来讲，RDD 具有以下几个属性。</p>
<ul>
<li>只读：不能修改，只能通过转换操作生成新的 RDD。</li>
<li>分布式：可以分布在多台机器上进行并行处理。</li>
<li>弹性：计算过程中内存不够时它会和磁盘进行数据交换。</li>
<li>基于内存：可以全部或部分缓存在内存中，在多次计算间重用。</li>
</ul>
<blockquote>
<p><strong>RDD、DataFrame 和 Dataset 的区别</strong></p>
<p>RDD 是 Spark 的基本数据结构，Dataframe 是 Spark 更高级的数据结构抽象，Dataset 是 DataFrame API 的扩展。<a href="https://www.cnblogs.com/lestatzhang/p/10611320.html">参考</a></p>
</blockquote>
<h3 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h3><p>SparkContext 是 spark 功能的主要入口，它代表与 spark 集群的连接，能够用来在集群上创建 RDD、累加器、广播变量。每个 JVM 里只能存在一个处于激活状态的 SparkContext，在创建新的 SparkContext 之前必须调用 stop()来关闭之前的 SparkContext。</p>
<p>SparkContext 在 spark 应用中起到了 master 的作用，掌控了所有 Spark 的生命活动，统筹全局，除了具体的任务在 executor 中执行，其他的任务调度、提交、监控、RDD 管理等关键活动均由 SparkContext 主体来完成。</p>
<p><img src="/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Spark-img-2.jpg"></p>
<h3 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h3><p>SparkSession 是 Spark-2.0 引入的新概念。SparkSession 为用户提供了统一的切入点，来让用户学习 Spark 的各项功能。</p>
<p>在 Spark 的早期版本中，SparkContext 是 Spark 的主要切入点，由于 RDD 是主要的 API，我们通过 sparkContext 来创建和操作 RDD。对于每个其他的 API，我们需要使用不同的 context。例如：</p>
<ol>
<li>对于 Spark Streaming，我们需要使用 StreamingContext</li>
<li>对于 Spark SQL，使用 SQLContext</li>
<li>对于 Hive，使用 HiveContext</li>
</ol>
<p>但是随着 DataSet 和 DataFrame 的 API 逐渐成为标准的 API，就需要为他们建立接入点。所以在 Spark2.0 中，引入 SparkSession 作为 DataSet 和 DataFrame API 的切入点，SparkSession 封装了 SparkConf、SparkContext 和 SQLContext。为了向后兼容，SQLContext 和 HiveContext 也被保存下来。所以我们现在实际写程序时，只需要定义一个 SparkSession 对象就可以了。</p>
<h2 id="操作模式"><a href="#操作模式" class="headerlink" title="操作模式"></a>操作模式</h2><ol>
<li>交互式 shell：<ol>
<li><code>spark-shell</code>：Spark Shell 是 Spark 预置的 REPL，默认的语言是 Scala。</li>
<li><code>pyspark</code>：Python 交互式 Shell</li>
<li><code>spark-sql</code>：sql 交互式 Shell</li>
</ol>
</li>
<li>编程接口：Spark 提供 Scala，Java，Python 等接口库。</li>
</ol>
<h2 id="作业提交"><a href="#作业提交" class="headerlink" title="作业提交"></a>作业提交</h2><blockquote>
<p>参考：<a href="http://spark.apache.org/docs/latest/submitting-applications.html">Submitting Applications</a></p>
</blockquote>
<p>Spark 应用程序作为集群上的独立进程集运行，由主程序（称为 driver program）中的 SparkContext 对象协调。</p>
<p>具体来说，为了在集群上运行，SparkContext 可以连接到多种类型的集群管理器（cluster managers ，如 Spark 自己的独立集群管理器、Mesos 或 YARN），它们在应用程序之间分配资源。 连接后，Spark 会在集群中的节点上获取执行程序，这些进程为您的应用程序运行计算和存储数据。 接下来，它将您的应用程序代码（由传递给 SparkContext 的 JAR 或 Python 文件定义）发送到执行程序（executors）。 最后，SparkContext 将任务发送给执行器运行。</p>
<h3 id="作业流程"><a href="#作业流程" class="headerlink" title="作业流程"></a>作业流程</h3><p>无论运行在哪种模式下，Spark 作业的执行流程都是相似的，主要有如下八步：</p>
<ol>
<li>客户端提交作业</li>
<li>Driver 启动流程</li>
<li>Driver 申请资源并启动其余 Executor(即 Container)</li>
<li>Executor 启动流程</li>
<li>作业调度，生成 stages 与 tasks。</li>
<li>Task 调度到 Executor 上，Executor 启动线程执行 Task 逻辑</li>
<li>Driver 管理 Task 状态</li>
<li>Task 完成，Stage 完成，作业完成</li>
</ol>
<h3 id="提交方法"><a href="#提交方法" class="headerlink" title="提交方法"></a>提交方法</h3><ol>
<li><p>spark-submit</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... <span class="comment"># other options</span></span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li><code>--class</code>：应用程序的主类，仅针对 java 或 scala 应用</li>
<li><code>--master</code>： master 的地址，提交任务到哪里执行，例如 spark:&#x2F;&#x2F;host:port, yarn, local</li>
<li><code>--deploy-mode</code>： 在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</li>
<li><code>--conf</code>：指定 spark 配置属性的值</li>
<li><code>application-jar</code>：</li>
<li><code>application-arguments</code>：传递给你的主函数的参数</li>
</ul>
<p>如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run a Python application on a Spark standalone cluster</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  examples/src/main/python/pi.py \</span><br><span class="line">  1000</span><br></pre></td></tr></table></figure>
</li>
<li><p>spark-sql</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-sql</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li><code>--master</code>：</li>
<li><code>--deploy-mode</code>：</li>
<li><code>--executor-memory</code>：</li>
<li><code>--driver-memory</code>：</li>
</ul>
<p>可以通过使用命令<code>spark-sql --help</code>查看命令的使用。</p>
</li>
</ol>
<h2 id="PySpark"><a href="#PySpark" class="headerlink" title="PySpark"></a>PySpark</h2><blockquote>
<p>参考：<a href="https://spark.apache.org/docs/latest/api/python/reference/index.html">API Reference</a></p>
</blockquote>
<p>PySpark 是 Spark 的 Python API。使用 PySpark，您也可以使用 Python 编程语言处理 RDD。</p>
<h3 id="操作模式-1"><a href="#操作模式-1" class="headerlink" title="操作模式"></a>操作模式</h3><ol>
<li>交互式 PySpark shell</li>
<li>使用 Python 运行</li>
</ol>
<h3 id="基本模块"><a href="#基本模块" class="headerlink" title="基本模块"></a>基本模块</h3><blockquote>
<p>参考：<a href="https://spark.apache.org/docs/latest/api/python/reference/index.html">API Reference</a></p>
</blockquote>
<p>PySpark 根据 Spark 的不同模块，也设计了如下的 7 个 API 接口模块：Spark Core，Spark Streaming，Spark SQL，Structured Streaming，MLlib (RDD-based)，MLlib (DataFrame-based)，Resource Management。</p>
<h4 id="Spark-Core-1"><a href="#Spark-Core-1" class="headerlink" title="Spark Core"></a>Spark Core</h4><blockquote>
<p>参考：<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD Programming Guide</a></p>
</blockquote>
<p>用以下代码创建存储一组单词的 RDD（spark 使用 parallelize 方法创建 RDD），我们现在将对单词进行一些操作。</p>
<h5 id="RDD-创建"><a href="#RDD-创建" class="headerlink" title="RDD 创建"></a>RDD 创建</h5><p>主要使用两种方式进行创建：</p>
<ol>
<li><code>sc.paralize()</code>：从上下文的 list，或者 array 中创建</li>
<li><code>sc.textFile()</code>：从文件进行创建，参数 1 是路径，参数 2 是代表数据集被划分的分区数。如果是本地文件的话，路径前面需要加上<code>file://</code>。</li>
</ol>
<h5 id="RDD-常用内置方法"><a href="#RDD-常用内置方法" class="headerlink" title="RDD 常用内置方法"></a>RDD 常用内置方法</h5><p>指的是对整个数据进行操作，因为常常要把很大的数据，处理成我们想要的格式。为了方便处理，所以设计了如下的接口：</p>
<ol>
<li><code>map(f, preservesPartitioning=False)</code><ul>
<li>参数：<ul>
<li><code>f: func</code>：自定义行处理函数，这个函数的输入参数是 RDD 中的一行，返回值是处理后结果。</li>
<li><code>preservesPartitioning: bool</code>：表示是否保留父 RDD 的 partitioner 分区信息。</li>
</ul>
</li>
<li>功能：将函数 f 应用在每一个 RDD 元素上，也就是对每一行做转换。</li>
</ul>
</li>
<li><code>reduce(f)</code><ul>
<li>参数：<ul>
<li><code>f: func</code>：自定义处理函数，参数固定有两个。</li>
</ul>
</li>
<li>功能：将 RDD 中元素两两传递给输入函数，同时产生一个新的值，新产生的值与 RDD 中下一个元素再被传递给输入函数直到最后只有一个值为止。</li>
</ul>
</li>
<li><code>reduceByKey(f, numPartitions, partitionFunc)</code><ul>
<li>参数：<ul>
<li><code>f: func</code>：自定义处理函数</li>
<li><code>numPartitions</code>：</li>
<li><code>partitionFunc</code>：</li>
</ul>
</li>
<li>功能：对元素为 KV 对的 RDD 中 Key 相同的元素的 Value 进行 reduce，因此，Key 相同的多个元素的值被 reduce 为一个值，然后与原 RDD 中的 Key 组成一个新的 KV 对。</li>
</ul>
</li>
<li><code>groupBy(f[, numPartitions, partitionFunc])</code><ul>
<li>参数：<ul>
<li><code>f: func</code>：自定义的处理函数</li>
<li><code>numPartitions</code>：</li>
<li><code>partitionFunc</code>：</li>
</ul>
</li>
<li>功能：接收一个函数，这个函数返回的值作为 key，然后通过这个 key 来对里面的元素进行分组，返回一个聚合的 RDD</li>
<li>例子：<code>rdd.groupBy(lambda x: x % 2).collect()</code></li>
</ul>
</li>
<li><code>groupByKey([numPartitions, partitionFunc])</code><ul>
<li>参数：<ul>
<li><code>numPartitions</code>：</li>
<li><code>partitionFunc</code>：</li>
</ul>
</li>
<li>功能：直接将键值对类型的数据的 key 作为 group 的 key 值，返回一个聚合的 RDD</li>
<li>例子：<code>rdd.groupByKey().mapValues(list).collect()</code></li>
</ul>
</li>
<li><code>filter(f)</code><ul>
<li>参数：<ul>
<li><code>f: func</code>：自定义行处理函数，返回值要求是布尔值。</li>
</ul>
</li>
<li>功能：过滤，在 RDD 中筛选符合特定条件的数据元素。</li>
</ul>
</li>
<li><code>flatMap(f, preservesPartitioning=False)</code>：<ul>
<li>参数：同 map</li>
<li>功能：与 map 类似，但返回的是一个扁平的结果，不是一个列表。</li>
</ul>
</li>
<li><code>foreach(f)</code><ul>
<li>参数：<ul>
<li><code>f: func</code>：</li>
</ul>
</li>
<li>功能：对这个 RDD 的每个元素都使用函数 f</li>
</ul>
</li>
<li><code>join(other, numPartitions=None)</code><ul>
<li>参数：<ul>
<li><code>other</code>：另一个 RDD</li>
</ul>
</li>
<li>功能：将这个 RDD 与另一个 RDD 通过 key 值连接，返回一个新的 RDD</li>
<li>例子：<code>x.join(y).collect()</code></li>
</ul>
</li>
<li><code>collect()</code><ul>
<li>功能：返回一个包含这个 RDD 所有元素的 list</li>
</ul>
</li>
<li><code>count()</code><ul>
<li>功能：返回这个 RDD 中的元素的数量</li>
</ul>
</li>
<li><code>distict(numPartitions=None)</code>：<ul>
<li>参数：<ul>
<li><code>numPartitions</code>：</li>
</ul>
</li>
<li>功能：去重，返回一个新的 RDD，这个 RDD 只包含父 RDD 不同的元素。</li>
</ul>
</li>
<li><code>sample(withReplacement, fraction, seed=None)</code>：<ul>
<li>参数：<ul>
<li><code>withReplacement: bool</code>：元素是否能够重复采样</li>
<li><code>fraction: float</code>：采样比率，必须在[0, 1]之间，采样出来的 RDD 大小是原来 RDD 大小的多少</li>
<li><code>seed: int optional</code>：随机种子</li>
</ul>
</li>
<li>功能：生成一个此 RDD 的采样子集</li>
</ul>
</li>
</ol>
<h4 id="Spark-SQL-1"><a href="#Spark-SQL-1" class="headerlink" title="Spark SQL"></a>Spark SQL</h4><blockquote>
<p>参考：<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL, DataFrames and Datasets Guide</a></p>
</blockquote>
<h4 id="相关聚合函数"><a href="#相关聚合函数" class="headerlink" title="相关聚合函数"></a>相关聚合函数</h4><p>库是<code>pyspark.sql.functions</code>，</p>
<ol>
<li>max</li>
<li>min</li>
<li>count</li>
<li>sum</li>
<li>avg</li>
<li>mean</li>
<li>sumDistinct：去重后求和</li>
<li>collect_list</li>
<li>collect_set</li>
<li>stddev</li>
<li>variance</li>
</ol>
<p>用法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.functions import collect_list</span><br><span class="line"></span><br><span class="line">after_data = data_df.groupBy(data_df.mobile).agg(collect_list(data_df.app_code))</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css?v=1.0.0"><p><span>本文标题：</span>Spark</p><p><span>文章作者：</span>Jinzhao Tian</p><p><span>发布时间：</span>2022-04-29</p><p><span>最后更新：</span>2022-04-29</p><p><span>原始链接：</span><a href="/Big-Data/大数据/Spark/">http://jinzhaotian.github.io/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="http://jinzhaotian.github.io/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/"></i></span></p><p><span>版权声明：</span>转载请注明出处！</p></div><br><div class="tags"><a href="/tags/Big Data"><i class="fa fa-tag">Big Data</i></a><a href="/tags/Spark"><i class="fa fa-tag">Spark</i></a></div><div class="post-nav"><a class="pre" href="/back-end/%E5%90%8E%E7%AB%AF/Docker/">Docker</a><a class="next" href="/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/">Hive</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Big-Data/">Big Data</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/back-end/">back end</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/basic-knowledge/">basic knowledge</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/back-end/%E5%90%8E%E7%AB%AF/Docker/">Docker</a></li><li class="post-list-item"><a class="post-list-link" href="/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/">Spark</a></li><li class="post-list-item"><a class="post-list-link" href="/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/">Hive</a></li><li class="post-list-item"><a class="post-list-link" href="/Big-Data/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/">Hadoop</a></li><li class="post-list-item"><a class="post-list-link" href="/DataBase/%E5%90%8E%E7%AB%AF/PostgreSQL/">PostgreSQL</a></li><li class="post-list-item"><a class="post-list-link" href="/basic-knowledge/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/ssh%E5%91%BD%E4%BB%A4/">ssh命令</a></li><li class="post-list-item"><a class="post-list-link" href="/basic-knowledge/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Linux%E7%B3%BB%E7%BB%9F/">Linux系统</a></li><li class="post-list-item"><a class="post-list-link" href="/basic-knowledge/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">Linux常用命令</a></li><li class="post-list-item"><a class="post-list-link" href="/basic-knowledge/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Git%E7%9B%B8%E5%85%B3/">Git相关</a></li><li class="post-list-item"><a class="post-list-link" href="/basic-knowledge/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Conda%E7%9B%B8%E5%85%B3/">Conda相关</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/JinzhaoTian" title="GitHub" target="_blank">GitHub</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">Jinzhao Tian.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
  search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>